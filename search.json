[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website hosts live resources used for the “Reproducible workflows” workshop, developed by Dr Ceres Barros with a lot of inspiration from other resources (see Thanks)\nThe workshop materials presented here were initially conceived for the BES Macroecology SIG meeting held in Birmingham, UK, July 12-13th, 2023.\nPowered by Quarto"
  },
  {
    "objectID": "about.html#workshop-facilitators",
    "href": "about.html#workshop-facilitators",
    "title": "About",
    "section": "Workshop facilitators",
    "text": "Workshop facilitators\n\nDr Ceres Barros\nCeres is a Research Ecologist at the Future Forest Ecosystems Centre, within the British Columbia Ministry of Forests, Canada. She has several years of experience in using complex ecological models to understand and forecast ecosystem responses to environmental change (e.g. climate change, fire disturbances, land-use changes, among others) both in applied and theoretical contexts. She has merged her passion to understand ecosystem stability, with her dedication to developing models and modelling tools that are grounded on data, reproducible, reusable and transparent, but also accessible to most ecologists. Her models and workflows are based in R (a programming language most ecologists understand), using SpaDES [a workflow manager, providing a modelling standard and toolkit in R; Chubaty and McIntire (n.d.)] and GitHub to promote collaboration with other model developers within and across disciplines.\nCeres lives in Victoria, BC."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible Workflows Workshops",
    "section": "",
    "text": "Welcome to the Reproducible Workflows Workshops website.\nHere you will find the workshops materials, including presentations, code and other useful information.\nThis is a live site and resources will be added and updated periodically.\n\n\n\nPowered by Quarto"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "BES Macroecology SIG meeting 2023, Birmingham UK\n\n\n\n\nPowered by Quarto"
  },
  {
    "objectID": "thanks.html",
    "href": "thanks.html",
    "title": "Thanks",
    "section": "",
    "text": "I am thankful to many researchers across the globe who have developed incredible tools and resources to both enable and teach the development of reproducible workflows. Amongst these, special thanks go to:\nPowered by Quarto"
  },
  {
    "objectID": "thanks.html#sponsors-and-supporters",
    "href": "thanks.html#sponsors-and-supporters",
    "title": "Thanks",
    "section": "Sponsors and supporters",
    "text": "Sponsors and supporters\nI am also grateful for the logistic and financial support from:\n\nThe Future Forest Ecosystems Centre and BC Ministry of Forests, Canada\nThe British Ecological Society"
  },
  {
    "objectID": "tutos.html",
    "href": "tutos.html",
    "title": "Tutorials & Examples",
    "section": "",
    "text": "I was asked multiple times why I picked species distribution models (SDMs) to teach reproducible workflows in R (for ecologists). Simple, climate-based SDMs may not be great to actually forecast species distributions, due to the several other processes they are missing (e.g. dispersal limitation and biotic interactions), but they can be great for hypothesis testing (see Lee-Yaw et al. 2021 for a quick overview of SDM strengths and weaknesses) and as educational tools. They are easy to understand, relatively fast to run in nowadays computes, have several R packages that make their fitting and validation easier, the data they need is often easily obtainable at large scales and have many features and requirements that are common with other ecological workflows:\nNeed different sources of data that should be as FAIR Wilkinson et al. (2016) as possible;\nOften require some degree of data preparation (e.g. GIS operations.) to put the data in the right format;\nRely on statistical steps like model fitting, model validation and running model prediction on the same or new data (be careful with climate-based SDMs and extrapolation, though…!);\nProduce visual outputs (e.g. maps).\nMy hope is that this example will can serve as a starting point for you to adapt your current workflows (regardless of what they are used for) and ground them on the R^3T principles.\nClick here to open the example on a new page.\nPowered by Quarto"
  },
  {
    "objectID": "tutos.html#reproducible-workflows-in-r",
    "href": "tutos.html#reproducible-workflows-in-r",
    "title": "Tutorials & Examples",
    "section": "",
    "text": "I was asked multiple times why I picked species distribution models (SDMs) to teach reproducible workflows in R (for ecologists). Simple, climate-based SDMs may not be great to actually forecast species distributions, due to the several other processes they are missing (e.g. dispersal limitation and biotic interactions), but they can be great for hypothesis testing (see Lee-Yaw et al. 2021 for a quick overview of SDM strengths and weaknesses) and as educational tools. They are easy to understand, relatively fast to run in nowadays computes, have several R packages that make their fitting and validation easier, the data they need is often easily obtainable at large scales and have many features and requirements that are common with other ecological workflows:\nNeed different sources of data that should be as FAIR Wilkinson et al. (2016) as possible;\nOften require some degree of data preparation (e.g. GIS operations.) to put the data in the right format;\nRely on statistical steps like model fitting, model validation and running model prediction on the same or new data (be careful with climate-based SDMs and extrapolation, though…!);\nProduce visual outputs (e.g. maps).\nMy hope is that this example will can serve as a starting point for you to adapt your current workflows (regardless of what they are used for) and ground them on the R^3T principles.\nClick here to open the example on a new page."
  },
  {
    "objectID": "tutos_examples/reproducible-workflows-example/global.html",
    "href": "tutos_examples/reproducible-workflows-example/global.html",
    "title": "Example of a reproducible workflow in R",
    "section": "",
    "text": "Repeatable, Reproducible, Reusable and Transparent (R^3T) workflows are becoming more and more commonly used in academic and non-academic settings to ensure that analyses and their outputs can be verified and repeated by peers, stakeholders and even the public. As such, (R^3T) promote trust within the scientific community and between scientists, stakeholders, end-users and the public. R^3T are also fundamental for model benchmarking, conducting meta-analyses and facilitating building-on and improving current scientific methods, especially those involving statistical and non-statistical modelling.\nIn the ecological domain, R^3T are gaining traction, but many of us struggle at the start, not knowing where to begin. This example will take you through setting up an R^3T workflow for a common analysis in ecology, species distribution modelling.\nThe workflow has been kept as simple as possible, while following a few common guidelines that facilitate building R^3T workflows:\n\nScripting;\nMinimising no. software/languages used;\nModularising & “functionising”;\nCentralising the workflow in a single script;\nUsing a self-contained & project-oriented workflow;\nUsing version control;\nUsing (minimal) integrated testing.\n\nIt also relies on two important packages that enable reproducibility and speed up subsequent runs (i.e. re-runs) the workflow in the same machine:\n\nRequire (McIntire 2023) - for self-contained and reproducible R package installation;\nreproducible (McIntire and Chubaty 2023) - for caching, establishing explicit links to raw data, common GIS operations, and more.\n\nHowever, please note that there are numerous other R packages (as well as other languages and types of software) that facilitate and support R^3T workflows. This is simply an example using some of the tools that I use when building my workflows.\nPowered by Quarto"
  },
  {
    "objectID": "tutos_examples/reproducible-workflows-example/global.html#introduction",
    "href": "tutos_examples/reproducible-workflows-example/global.html#introduction",
    "title": "Example of a reproducible workflow in R",
    "section": "",
    "text": "Repeatable, Reproducible, Reusable and Transparent (R^3T) workflows are becoming more and more commonly used in academic and non-academic settings to ensure that analyses and their outputs can be verified and repeated by peers, stakeholders and even the public. As such, (R^3T) promote trust within the scientific community and between scientists, stakeholders, end-users and the public. R^3T are also fundamental for model benchmarking, conducting meta-analyses and facilitating building-on and improving current scientific methods, especially those involving statistical and non-statistical modelling.\nIn the ecological domain, R^3T are gaining traction, but many of us struggle at the start, not knowing where to begin. This example will take you through setting up an R^3T workflow for a common analysis in ecology, species distribution modelling.\nThe workflow has been kept as simple as possible, while following a few common guidelines that facilitate building R^3T workflows:\n\nScripting;\nMinimising no. software/languages used;\nModularising & “functionising”;\nCentralising the workflow in a single script;\nUsing a self-contained & project-oriented workflow;\nUsing version control;\nUsing (minimal) integrated testing.\n\nIt also relies on two important packages that enable reproducibility and speed up subsequent runs (i.e. re-runs) the workflow in the same machine:\n\nRequire (McIntire 2023) - for self-contained and reproducible R package installation;\nreproducible (McIntire and Chubaty 2023) - for caching, establishing explicit links to raw data, common GIS operations, and more.\n\nHowever, please note that there are numerous other R packages (as well as other languages and types of software) that facilitate and support R^3T workflows. This is simply an example using some of the tools that I use when building my workflows."
  },
  {
    "objectID": "tutos_examples/reproducible-workflows-example/global.html#the-workflow-at-a-glance",
    "href": "tutos_examples/reproducible-workflows-example/global.html#the-workflow-at-a-glance",
    "title": "Example of a reproducible workflow in R",
    "section": "The workflow at a glance",
    "text": "The workflow at a glance\nIn this example, I use a simple statistical framework to predict climate-driven range changes of white spruce (Picea glauca), a common tree species in Canada. It should, by no means, be interpreted as a good way to project changes in the actual distributions of white spruce, which will likely involve many more drivers and ecological processes than the ones included here.l\nTo keep the example simple, I focused on white spruce distribution changes within the province of British Columbia (BC) in response to variation in four bioclimatic predictors (obtained from WorldClim, Fick and Hijmans 2017):\n\nBIO1 - Annual Mean Temperature;\nBIO4 - Temperature Seasonality;\nBIO12 - Annual Precipitation;\nBIO15 - Precipitation Seasonality.\n\nThe baseline white spruce distribution for BC was obtained from the 2011 species % cover maps by Beaudoin et al. (2014), freely available at the Canadian National Forest Inventory database. The baseline values for each bioclimatic variable corresponded to projections for the normal climate period of 1970-2010 (hereafter, 2010), while future values where obtained from projections for the periods 2021-2040 (2030), 2041-2060 (2050), 2061-2080 (2070) and 2081-2100 (2090), using the CanESM5 General Circulation Model and the SSP 585 emissions scenario.\nIn this workflow, I use a single random forest model (randomForest package, Liaw and Wiener 2002) to model white spruce range changes as a function of the four bioclimatic variables.\nAll steps from sourcing and formatting the data, to evaluating and plotting model results, are integrated in the workflow as a sequence of sourced scripts. This method was chosen as it is likely closer to the way most of us learn of to use R. However, other approaches could also be used, for instance:\n\nconverting existing scripts to single or multiple functions called from the controller script (see Running the workflow);\nadapting scripts to SpaDES (Chubaty and McIntire 2019, see the SpaDES4Dummies guide) modules and using SpaDES to execute the workflow;\nusing targets (see Brousil et al. 2023) to execute the workflow;\nand more…\n\nThe degree to which a workflow developer will encapsulate code into separate scripts, functions or modules/components of an automated workflow using packages like SpaDES/targets, will to some level depend on individual preference and the end users of the workflow. For many “typical” users of R (at least in ecology) it feels strange or even hard to break up a workflow into different scripts/functions/etc. and some have told me they feel it’s “harder to debug and understand a script if I can’t see it easily”. However, before you decide to leave lots of code in your “main” (or controller) script I urge you to think about:\n\nWhy are you leaving the code there? Is it absolutely essential to understand that piece to execute the workflow or understand the outputs? Or it is something like a data preparation step that is meaningless for interpretation and therefore could be put in the background away?\nWho is going to use your script? Only you? If so, you should know all the pieces and can keep the controller script neat and tidy for production runs. Or do you have teaching in mind and therefore want a longer script that could be easier for “R newbies” – for instance, in the workflow below I left the package installation steps in the controller script because I wanted to make sure you saw them. If your end-user is someone like a stakeholder, they may also not care as much about the workflow details as you do, and a simpler, cleaner script may be easier to run and be last prone to error (e.g. forgetting to run a line).\nIf your problem is debugging, please see Debugging for tips that will make your life easier and potentially remove the necessity of very long scripts containing both function definitions and function calls."
  },
  {
    "objectID": "tutos_examples/reproducible-workflows-example/global.html#step-by-step",
    "href": "tutos_examples/reproducible-workflows-example/global.html#step-by-step",
    "title": "Example of a reproducible workflow in R",
    "section": "Step by step",
    "text": "Step by step\n\nProject structure and dependencies\nA workflow should start with the initial set-up necessary to execute all operations. In R this mostly concerns the R version, setting up project directories and package installation and loading. Other examples are options() (set afterwards in this workflow – Functions, options and study area) or any necessary system variables (not used here).\nNote that in self-contained workflows all packages should be installed at the project level to avoid conflicts with (or changing) the system and/or user R library.\n\n## an assertion:\nif (getRversion() &lt; \"4.3\") {\n  warning(paste(\"This workflow was build using R v4.3.0;\",\n                \"it is recommended you update your R version to v4.3.0 or above\"))\n}\n\n\n## package installation with Require\n\n## create project folder structure\nprojPaths &lt;- list(\"pkgPath\" = file.path(\"packages\", version$platform,\n                                        paste0(version$major, \".\", strsplit(version$minor, \"[.]\")[[1]][1])),\n                  \"cachePath\" = \"cache\",\n                  \"codePath\" = \"code\",\n                  \"dataPath\" = \"data\",\n                  \"figPath\" = \"figures\",\n                  \"outputsPath\" = \"outputs\")\nlapply(projPaths, dir.create, recursive = TRUE, showWarnings = FALSE)\n\n## set R main library to project library\n.libPaths(projPaths$pkgPath)\n\nI use Require to install all necessary packages. Require can install a mix of CRAN and GitHub packages, and accepts specification of minimum package versions (see ?Require). For simplicity sake, this example uses the latest versions of each package from CRAN.\nAnother advantage of using Require is that it can save a “package snapshot” (i.e. the state of an R library at a particular time) and use it at a later date to install packages using the versions (and sources - CRAN or GitHub) specified in the snapshot file. This ensures that the same library state can be recreated in the future or on another machine.\nTo demonstrate this, and because it is an essential piece of workflow reproducibility, the code below shows both the initial package installation and saving of the snapshot file – commented to avoid unintended package/snapshot updates during subsequent runs – and package installation using the snapshot file.\n/!\\ You will need an active internet connection to run (or re-run) the package installation lines /!\\\n\n## install specific versions of packages\ninstallRequire &lt;- !\"Require\" %in% rownames(installed.packages())\ninstallRequire &lt;- if (!installRequire) packageVersion(\"Require\") &lt; \"0.3.0\" else installRequire\nif (installRequire) {\n  install.packages(\"Require\", dependencies = TRUE)\n}\n\nlibrary(Require)\n\n## this may take a while.\n## Notes for Windows users: failure occurs often because the OS can \"hold\" on to folders.\n##    Don't despair. Restart R session, close all other sessions and run the script up to\n##    this point again.\n# loadOut &lt;- Require(c(\"data.table\", \"dismo\",\n#                      \"ggplot2\", \"httr\", \"maps\",\n#                      \"randomForest\",\n#                      \"rasterVis\", \"reproducible\", \n#                      \"terra\"),\n#                    standAlone = TRUE)\n\n## eventually we would save the library state in a package snapshot file\npkgSnapshotFile &lt;- file.path(projPaths$pkgPath, \"pkgSnapshot.txt\")\n# pkgSnapshot(\n#   packageVersionFile = file.path(projPaths$pkgPath, \"pkgSnapshot.txt\"),\n#   libPaths = projPaths$pkgPath, standAlone = TRUE\n# )\n## and replace the above Require() call with\nRequire(packageVersionFile = pkgSnapshotFile, \n        standAlone = TRUE,\n        libPaths = projPaths$pkgPath,\n        upgrade = FALSE, require = FALSE)\n\nRequire(c(\"data.table\", \"dismo\",\n          \"ggplot2\", \"httr\", \"maps\",\n          \"randomForest\",\n          \"rasterVis\", \"reproducible\",\n          \"terra\"),\n        install = FALSE)\n\n\ninstall.packages(\"Require\", dependencies = TRUE)\nlibrary(Require)\nRequire(c(\"data.table\", \"dismo\",\n          \"ggplot2\", \"httr\", \"maps\",\n          \"randomForest\",\n          \"rasterVis\", \"reproducible\",\n          \"terra\"),\n        standAlone = TRUE)\n\nIn some Windows machines the package installation lines above may have to be run multiple times, with R session “refreshes” between them. This is because, Windows often “holds on” to temporary folders created during package installation, preventing R from manipulating these folders and causing installation failures. I’m afraid you’ll simply have to be patient here.\n\n\nRunning the workflow\nThe workflow presented here is relatively simple. It has been broken down into a series of .R scripts sourced in sequence. In the past, I found this to be the most intuitive way of learning how to shift from my spaghetti code into something structured and more robust. However, there are many other ways in which the workflow could have been set up. For instance, each of the scripts could be turned into a function and the functions called here; or packages like SpaDES and targets could be used to set and manage the workflow (see The workflow at a glance). The choice will depend on project complexity and on the level of programming proficiency of the developer, among other factors.\n\n## workflow functions/options and study area \nsource(file.path(projPaths$codePath, \"miscFuns.R\"))\nsource(file.path(projPaths$codePath, \"setup.R\"))\n\n## download data\nsource(file.path(projPaths$codePath, \"climateData.R\"))\nsource(file.path(projPaths$codePath, \"speciesData.R\"))\n\n## fit SDM\nsource(file.path(projPaths$codePath, \"fitSDM.R\"))\n\n## obtain projections\nsource(file.path(projPaths$codePath, \"projections.R\"))\n\n\nFunctions, options and study area\nThe first script, miscFuns.R, contains several functions used across other scripts. It is good practice to document functions – what they do, what parameters they expect, and what packages they depend on. As in many R packages, I use the roxygen2 format, where function documentation is preceded by #' and documentation fields are noted by a @. Please see this quickstart guide to roxygen2 to learn more.\nHere’s an example taken from one of the functions in miscFuns.R:\n\n#' Plot a `SpatRaster` as a `ggplot`\n#'\n#'\n#' @param ras a SpatRaster layer\n#' @param title character. Plot title\n#' @param xlab character. X-axis title\n#' @param ylab character. Y-axis title\n#' @param isDiscrete logical. Should raster data be treated as discrete\n#'   or continuous for plotting? If `TRUE` plots will be accompanied with\n#'   a colour legend and only existing values. Otherwise, a continuous\n#'   colourbar is shown (default).\n#'\n#' @return a `ggplot`.\n#'\n#' @importFrom rasterVis gplot\n#' @importFrom ggplot2 geom_tile scale_fill_brewer coord_equal theme_bw\nplotSpatRaster &lt;- function(ras, plotTitle = \"\", xlab = \"x\", ylab = \"y\", isDiscrete = FALSE) {\n  ...\n}\n\nThe second script of the workflow, setup.R, prepares the study area and sets some global options(). Notice that at the end of this (and other) script(s) I remove objects that are no longer necessary. Although this workflow will not demand a lot of memory, it is good practice to avoid spending memory resources on useless objects and cluttering the environment where the workflow is running – in this case the .GlobalEnv.\n\n\nData sourcing and preparation\nThe second and third scripts, climateData.R and speciesData.R, source and prepare bioclimatic data layers and the white spruce percent cover layer (later converted to presence/absence values) necessary to fit the species distribution model (SDM) and to project changes in species distributions. To ensure that the workflow is reproducible and transparent, I have used FAIR data (Wilkinson et al. 2016) explicitly linked to the workflow and prepared on-the-fly by the prepInputs function (reproducible package).\nprepInputs downloads the data from the specified URL and when the data is a spatial layer it can crop it and re-project it to match another spatial layer (the studyAreaRas created in setup.R). It will only download the data once and uses internal caching to speed up subsequent runs with the same inputs (e.g. same input data layer and study area). You will also note that I have used outer Cache calls (also from the reproducible package) for further speed up subsequent workflow runs (see Caching).\nThese data preparation scripts save plots of the final climate and species distribution data layers.\n\n\nAnalytical steps - model fitting, validation and projections\nThe scripts fitSDM.R and projections.R fit, validate and use a random forest (RF) model that relates baseline (from 2011) white spruce presences and absences (derived from the input percent cover raster layer) to the four bioclimatic variables. The model is fit on 80% of the data and tested on the remaining 20% – note the use of set.seed() to ensure that the data partitioning is repeatable.\nI used tools from the dismo package to create the training and testing data partitions, to evaluate the model and to predict changes in white spruce distributions under projected climate conditions for 2030, 2050, 2070 and 2090. I also used Cache again to avoid refitting and re-evaluating the model if inputs are not changed.\n\n\nOutputs\nThe workflow’s outputs are the RF model object, its evaluation results and plots of the raster layer projections. Instead of creating a separate script to save outputs to disk, I have embedded these operations within each script. I prefer saving outputs as they are being created, but in this case this is more a matter of personal taste. In very long workflows, however, it is advisable to save outputs as they are created to avoid losing work should there be a failure preventing workflow completion. See additional notes on Saving.\n\n\n\n\n\nFigure 1: Random forest projections of white spruce distributions in BC, Canada, under baseline conditions of four bioclimatic variables (annual mean temperature and annual precipitation, temperature and precipitation seasonality.\n\n\n\n\n\n\n\n\n\nFigure 2: Random forest projections of white spruce distributions in BC, Canada, for future values of the same four bioclimatic variables – only one GCM and one emissions scenario were used.\n\n\n\n\n\n\nAnd voilà!\nThat’s it!\nI now invite you to make further changes to the workflow. Some ideas are to make it so that you can control certain inputs (e.g. climate scenarios and periods) from the controller script, or turning the existing scripts into functions, or even using other workflow structures and packages like SpaDES and targets, which are beyond the scope of this example (but see SpaDES4Dummies guide and Brousil et al. 2023).\nI hope this minimal example has given useful tools and ideas that will help you create R^3T workflows and enhance your existing work with these principles."
  },
  {
    "objectID": "tutos_examples/reproducible-workflows-example/global.html#a-note-on",
    "href": "tutos_examples/reproducible-workflows-example/global.html#a-note-on",
    "title": "Example of a reproducible workflow in R",
    "section": "A note on…",
    "text": "A note on…\n\nDebugging\nHow many of you have created the arguments of a “problematic” function as objects on your .GlobalEnv to then execute a copy-pasted version of the said function line by line? If I had to guess I’d say too many! And I was doing that too – because there are too many R things we don’t learn on that stats. course in uni! But there are FAR better ways of debugging in R. Two common ones are to:\n1. place a browser() call on your “problematic” function\n\nplotSpatRaster &lt;- function(ras, plotTitle = \"\", xlab = \"x\", ylab = \"y\", isDiscrete = FALSE) {\n  plotOut &lt;- gplot(ras) +\n    geom_tile(aes(fill = value))\n  plotOut &lt;- if (isDiscrete) {\n    vals &lt;- na.omit(unique(as.vector(ras[])))\n    plotOut +\n      scale_fill_distiller(palette = \"Blues\", direction = 1,\n                           na.value = \"grey90\", guide = \"legend\",\n                           breaks = vals, limits = vals)\n  } else {\n    plotOut +\n      scale_fill_distiller(palette = \"Blues\", direction = 1, na.value = \"grey90\")\n  }\n  browser() ## debugging mode will start here, once the function is executed\n  plotOut +\n    theme_classic() +\n    coord_equal() +\n    labs(title = plotTitle, x = xlab, y = ylab, fill = \"\")\n}\n\n2. to use debug(&lt;function_name&gt;) and debugOnce(&lt;function_name&gt;) to execute the function in debugging mode (like putting a browser() call right at the start of the function):\n\ndebugOnce(plotSpatRaster)\nplotSpatRaster(ras)  ## the entire function will executed in debuging mode, only once\n\ndebug(plotSpatRaster)  ## always execute in debugging mode\nplotSpatRaster(ras)  \nundebug(plotSpatRaster)  ## stop debugging\n\nThese two functions will enable you to “enter” the function environment as it’s being executed and have access to the objects/arguments that the function “sees”, making debugging SO much easier.\n\n\nCaching\nCaching has been my life saviour for many years now. When you do a lot of coding and your code takes a while to run, debugging and development can become very slow (and frustrating) if you need to wait many minutes (many times) to get to the point of interest in the code. To avoid this, many of us are actually doing “manual” caching – we are saving intermediate objects to disk, checking whether they exist and when they do, bypassing re-running computations and loading them into R.\nThe problem is that this is not a robust solution – it is not sufficient to check whether the outputs exist – and others (i.e. computer programmers) have had more elegant solutions for a while. Cache (reproducible package) makes these elegant solutions easy to use by non-computer-scientists. On the first run of a function call, Cache will save its outputs and sufficient information about the function’s inputs and code. When the function call is run a second time, Cache will first check whether the outputs have been cached (i.e. saved), and whether the inputs and code have changed, and load the saved outputs if they haven’t.\nAs with most solutions, there are always trade-offs to consider. The first is a time trade-off. If the call being cached is very fast to run, the extra time used for caching, checking and loading will probably not compensate. One thing to consider is to not cache very large inputs (i.e. the objects passed to arguments) of a function (see omitArgs argument of Cache), as they will take longer to digest. Instead pass a smaller object that is unique to the large object to the cacheExtra argument.\n\nsystem.time(Cache(runif, n = 10))\nsystem.time(runif(n = 10)) ## will always be faster\n\nThe second is disk space. If you have very limited disk space, it may not be a good idea to cache several large intermediate files, as your disk may be filled up quickly, especially when working across many projects. Solutions are to only cache the slowest steps or to turn caching off (see useCache argument of Cache) in situations when you know disk spade will be limited (e.g. sharing your code with someone working with a small disk).\n\n\nSaving\nAs a rule of thumb, I avoid saving multiple objects in a single file (e.g. an .RData or .rda file), as it can reduce workflow robustness – if that file becomes corrupted several pieces or outputs of the workflow may be lost. Instead, I recommend saving/loading single objects as .rds or .qs (from qs package) files (see ?saveRDS and ?readRDS and ?qs::qsave and ?qs::qread). Cache will, by default, use the .rds format, but it is possible to opt for the .qs format by setting options(\"reproducible.cacheSaveFormat\" = \"qs\"). The advantages of using .qs are a higher compression rate and faster read/write speeds, the disadvantage is that your workflow will depend on an extra package."
  },
  {
    "objectID": "tutos_examples/reproducible-workflows-example/global.html#acknowledgements",
    "href": "tutos_examples/reproducible-workflows-example/global.html#acknowledgements",
    "title": "Example of a reproducible workflow in R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to acknowledge all participants (guinea-pigs, *ahem*) of the ‘Coding SOS: Reproducible workflows, GitHub, and R Open Mic’ workshop of the 2023 BES Macroecology SIG meeting, Birmingham UK, who through their curiosity and thoughtful questions have helped improve the contents of this example workflow."
  },
  {
    "objectID": "tutos_examples/reproducible-workflows-example/LICENSE.html",
    "href": "tutos_examples/reproducible-workflows-example/LICENSE.html",
    "title": "Reproducible Workflows Workshops",
    "section": "",
    "text": "CC0 1.0 Universal\nStatement of Purpose\nThe laws of most jurisdictions throughout the world automatically confer exclusive Copyright and Related Rights (defined below) upon the creator and subsequent owner(s) (each and all, an “owner”) of an original work of authorship and/or a database (each, a “Work”).\nCertain owners wish to permanently relinquish those rights to a Work for the purpose of contributing to a commons of creative, cultural and scientific works (“Commons”) that the public can reliably and without fear of later claims of infringement build upon, modify, incorporate in other works, reuse and redistribute as freely as possible in any form whatsoever and for any purposes, including without limitation commercial purposes. These owners may contribute to the Commons to promote the ideal of a free culture and the further production of creative, cultural and scientific works, or to gain reputation or greater distribution for their Work in part through the use and efforts of others.\nFor these and/or other purposes and motivations, and without any expectation of additional consideration or compensation, the person associating CC0 with a Work (the “Affirmer”), to the extent that he or she is an owner of Copyright and Related Rights in the Work, voluntarily elects to apply CC0 to the Work and publicly distribute the Work under its terms, with knowledge of his or her Copyright and Related Rights in the Work and the meaning and intended legal effect of CC0 on those rights.\n\nCopyright and Related Rights. A Work made available under CC0 may be protected by copyright and related or neighboring rights (“Copyright and Related Rights”). Copyright and Related Rights include, but are not limited to, the following:\n\n\nthe right to reproduce, adapt, distribute, perform, display, communicate, and translate a Work;\nmoral rights retained by the original author(s) and/or performer(s);\npublicity and privacy rights pertaining to a person’s image or likeness depicted in a Work;\nrights protecting against unfair competition in regards to a Work, subject to the limitations in paragraph 4(a), below;\nrights protecting the extraction, dissemination, use and reuse of data in a Work;\ndatabase rights (such as those arising under Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, and under any national implementation thereof, including any amended or successor version of such directive); and\nother similar, equivalent or corresponding rights throughout the world based on applicable law or treaty, and any national implementations thereof.\n\n\nWaiver. To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer’s Copyright and Related Rights and associated claims and causes of action, whether now known or unknown (including existing as well as future claims and causes of action), in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “Waiver”). Affirmer makes the Waiver for the benefit of each member of the public at large and to the detriment of Affirmer’s heirs and successors, fully intending that such Waiver shall not be subject to revocation, rescission, cancellation, termination, or any other legal or equitable action to disrupt the quiet enjoyment of the Work by the public as contemplated by Affirmer’s express Statement of Purpose.\nPublic License Fallback. Should any part of the Waiver for any reason be judged legally invalid or ineffective under applicable law, then the Waiver shall be preserved to the maximum extent permitted taking into account Affirmer’s express Statement of Purpose. In addition, to the extent the Waiver is so judged Affirmer hereby grants to each affected person a royalty-free, non transferable, non sublicensable, non exclusive, irrevocable and unconditional license to exercise Affirmer’s Copyright and Related Rights in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “License”). The License shall be deemed effective as of the date CC0 was applied by Affirmer to the Work. Should any part of the License for any reason be judged legally invalid or ineffective under applicable law, such partial invalidity or ineffectiveness shall not invalidate the remainder of the License, and in such case Affirmer hereby affirms that he or she will not (i) exercise any of his or her remaining Copyright and Related Rights in the Work or (ii) assert any associated claims and causes of action with respect to the Work, in either case contrary to Affirmer’s express Statement of Purpose.\nLimitations and Disclaimers.\n\n\nNo trademark or patent rights held by Affirmer are waived, abandoned, surrendered, licensed or otherwise affected by this document.\nAffirmer offers the Work as-is and makes no representations or warranties of any kind concerning the Work, express, implied, statutory or otherwise, including without limitation warranties of title, merchantability, fitness for a particular purpose, non infringement, or the absence of latent or other defects, accuracy, or the present or absence of errors, whether or not discoverable, all to the greatest extent permissible under applicable law.\nAffirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person’s Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work.\nAffirmer understands and acknowledges that Creative Commons is not a party to this document and has no duty or obligation with respect to this CC0 or use of the Work.\n\nFor more information, please see http://creativecommons.org/publicdomain/zero/1.0/\n\n\n\nPowered by Quarto"
  },
  {
    "objectID": "workshop-dates.html",
    "href": "workshop-dates.html",
    "title": "Workshop dates",
    "section": "",
    "text": "2023 BES Macroecology SIG annual meeting - MacroBrum 2023 - July 12th, 2023, Birmingham UK\nPowered by Quarto"
  },
  {
    "objectID": "workshop-dates.html#up-coming-workshops",
    "href": "workshop-dates.html#up-coming-workshops",
    "title": "Workshop dates",
    "section": "",
    "text": "2023 BES Macroecology SIG annual meeting - MacroBrum 2023 - July 12th, 2023, Birmingham UK"
  }
]